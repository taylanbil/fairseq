diff --git a/fairseq/data/concat_dataset.py b/fairseq/data/concat_dataset.py
index 0376b3ae..b91ba79e 100644
--- a/fairseq/data/concat_dataset.py
+++ b/fairseq/data/concat_dataset.py
@@ -8,7 +8,8 @@
 import numpy as np
 from torch.utils.data.dataloader import default_collate
 
-from . import FairseqDataset
+from . import FairseqDataset, plasma_utils
+from fairseq.data.resampling_dataset import ResamplingDataset
 
 
 class ConcatDataset(FairseqDataset):
@@ -21,7 +22,7 @@ def cumsum(sequence, sample_ratios):
             s += curr_len
         return r
 
-    def __init__(self, datasets, sample_ratios=1):
+    def __init__(self, datasets, sample_ratios=1, epoch=0, safe_to_remove_underlying_sizes=False):
         super(ConcatDataset, self).__init__()
         assert len(datasets) > 0, "datasets should not be an empty iterable"
         self.datasets = list(datasets)
@@ -30,6 +31,9 @@ def __init__(self, datasets, sample_ratios=1):
         self.sample_ratios = sample_ratios
         self.cumulative_sizes = self.cumsum(self.datasets, sample_ratios)
         self.real_sizes = [len(d) for d in self.datasets]
+        self._safe_to_remove_underlying_sizes = safe_to_remove_underlying_sizes
+        self._init_sizes()
+        self._epoch = epoch
 
     def __len__(self):
         return self.cumulative_sizes[-1]
@@ -62,7 +66,8 @@ def size(self, idx: int):
         return self.datasets[dataset_idx].size(sample_idx)
 
     def num_tokens(self, index: int):
-        return np.max(self.size(index))
+        # return np.max(self.size(index))
+        return self.sizes[index]
 
     def attr(self, attr: str, index: int):
         dataset_idx = bisect.bisect_right(self.cumulative_sizes, index)
@@ -70,6 +75,9 @@ def attr(self, attr: str, index: int):
 
     @property
     def sizes(self):
+        return self._sizes.array
+
+    def _init_sizes(self):
         _dataset_sizes = []
         for ds, sr in zip(self.datasets, self.sample_ratios):
             if isinstance(ds.sizes, np.ndarray):
@@ -78,7 +86,15 @@ def sizes(self):
                 # Only support underlying dataset with single size array.
                 assert isinstance(ds.sizes, list)
                 _dataset_sizes.append(np.tile(ds.sizes[0], sr))
-        return np.concatenate(_dataset_sizes)
+        self._sizes = plasma_utils.PlasmaArray(np.concatenate(_dataset_sizes))
+
+        # Hack to free up memory (only for multilingual_masked_lm)
+        if self._safe_to_remove_underlying_sizes:
+            for ds in self.datasets:
+                if isinstance(ds, ResamplingDataset):
+                    del ds.dataset.sizes[0]
+            # del ds.sizes[0]
+        # self._sizes = np.concatenate(_dataset_sizes)
 
     @property
     def supports_prefetch(self):
@@ -99,7 +115,11 @@ def prefetch(self, indices):
             frm = to
 
     def set_epoch(self, epoch):
+        if self._epoch == epoch:
+            return
+
         super().set_epoch(epoch)
         for ds in self.datasets:
             if hasattr(ds, 'set_epoch'):
                 ds.set_epoch(epoch)
+        self._init_sizes()
diff --git a/fairseq/data/iterators.py b/fairseq/data/iterators.py
index 813d9cba..9973619e 100644
--- a/fairseq/data/iterators.py
+++ b/fairseq/data/iterators.py
@@ -201,7 +201,7 @@ def next_epoch_itr(self, shuffle=True, fix_batches_to_gpus=False):
             self._cur_epoch_itr = self._get_iterator_for_epoch(
                 self.epoch, shuffle, fix_batches_to_gpus=fix_batches_to_gpus,
             )
-        self.dataset.set_epoch(self.epoch)
+        # self.dataset.set_epoch(self.epoch)
         return self._cur_epoch_itr
 
     def end_of_epoch(self) -> bool:
diff --git a/fairseq/tasks/fairseq_task.py b/fairseq/tasks/fairseq_task.py
index 5438a2f0..6e01f23e 100644
--- a/fairseq/tasks/fairseq_task.py
+++ b/fairseq/tasks/fairseq_task.py
@@ -156,6 +156,7 @@ def get_batch_iterator(
             required_batch_size_multiple=required_batch_size_multiple,
         )
 
+        print("| Created batch samples")
         # return a reusable, sharded iterator
         epoch_iter = iterators.EpochBatchIterator(
             dataset=dataset,
diff --git a/fairseq/tasks/multilingual_masked_lm.py b/fairseq/tasks/multilingual_masked_lm.py
index cc1e2331..9ea1b8b5 100644
--- a/fairseq/tasks/multilingual_masked_lm.py
+++ b/fairseq/tasks/multilingual_masked_lm.py
@@ -120,10 +120,10 @@ def load_dataset(self, split, epoch=0, combine=False, **kwargs):
         assert len(paths) > 0
         data_path = paths[epoch % len(paths)]
 
-        languages = [
+        languages = sorted([
             name for name in os.listdir(data_path)
             if os.path.isdir(os.path.join(data_path, name))
-        ]
+        ])
         print("| Training on {0} languages: {1}".format(len(languages), languages))
         print("| Language to id mapping: ", {
                 lang: id for id, lang in enumerate(languages)
@@ -194,12 +194,17 @@ def load_dataset(self, split, epoch=0, combine=False, **kwargs):
             )
             lang_datasets.append(lang_dataset)
 
+        dataset_lengths = np.array(
+            [len(d) for d in lang_datasets],
+            dtype=float,
+        )
+        print(
+            '| loaded total {} blocks for all languages'.format(
+                dataset_lengths.sum(),
+            )
+        )
         if split == self.args.train_subset:
             # For train subset, additionally up or down sample languages.
-            dataset_lengths = np.array(
-                [len(d) for d in lang_datasets],
-                dtype=float,
-            )
             sample_probs = self._get_sample_prob(dataset_lengths)
             print("| Sample probability by language: ", {
                     lang: "{0:.4f}".format(sample_probs[id])
@@ -223,9 +228,13 @@ def load_dataset(self, split, epoch=0, combine=False, **kwargs):
                 )
                 for i, d in enumerate(lang_datasets)
             ]
-            dataset = ConcatDataset(resampled_lang_datasets)
+            dataset = ConcatDataset(
+                resampled_lang_datasets,
+                epoch=epoch,
+                safe_to_remove_underlying_sizes=':' in self.args.data,
+            )
         else:
-            dataset = ConcatDataset(lang_datasets)
+            dataset = ConcatDataset(lang_datasets, epoch=epoch)
             lang_splits = [split]
             for lang_id, lang_dataset in enumerate(lang_datasets):
                 split_name = split + '_' + languages[lang_id]
@@ -286,12 +295,14 @@ def get_batch_iterator(
     ):
         # Recreate epoch iterator every epoch cause the underlying
         # datasets are dynamic due to sampling.
-        self.dataset_to_epoch_iter = None
-        return super().get_batch_iterator(
+        self.dataset_to_epoch_iter = {}
+        epoch_iter = super().get_batch_iterator(
             dataset, max_tokens, max_sentences, max_positions,
             ignore_invalid_inputs, required_batch_size_multiple,
             seed, num_shards, shard_id, num_workers, epoch,
         )
+        self.dataset_to_epoch_iter = {}
+        return epoch_iter
 
     @property
     def source_dictionary(self):
diff --git a/train.py b/train.py
index 435c35d3..d2240471 100644
--- a/train.py
+++ b/train.py
@@ -99,6 +99,10 @@ def main(args, init_distributed=False):
 
         reload_dataset = ':' in getattr(args, 'data', '')
         # sharded data: get train iterator for next epoch
+
+        # Hack delete previous iters
+        del epoch_itr._cur_epoch_itr.itr
+
         epoch_itr = trainer.get_train_iterator(epoch_itr.epoch, load_dataset=reload_dataset)
     train_meter.stop()
     print('| done training in {:.1f} seconds'.format(train_meter.sum))
@@ -123,11 +127,12 @@ def train(args, trainer, task, epoch_itr):
     extra_meters = collections.defaultdict(lambda: AverageMeter())
     valid_subsets = args.valid_subset.split(',')
     max_update = args.max_update or math.inf
+
+    print("| starting training epoch: {0}".format(epoch_itr.epoch))
     for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch):
         log_output = trainer.train_step(samples)
         if log_output is None:
             continue
-
         # log mid-epoch stats
         stats = get_training_stats(trainer)
         for k, v in log_output.items():
@@ -160,6 +165,7 @@ def train(args, trainer, task, epoch_itr):
 
     # log end-of-epoch stats
     stats = get_training_stats(trainer)
+
     for k, meter in extra_meters.items():
         stats[k] = meter.avg
     progress.print(stats, tag='train', step=stats['num_updates'])
@@ -238,16 +244,19 @@ def validate(args, trainer, task, epoch_itr, subsets):
 
         for sample in progress:
             log_output = trainer.valid_step(sample)
-
             for k, v in log_output.items():
-                if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']:
+                if k in ['loss', 'nll_loss', 'ntokens', 'nsentences']:
                     continue
+                if k == 'accuracy':
+                    extra_meters[k].update(v, log_output['sample_size'])
                 extra_meters[k].update(v)
 
         # log validation stats
         stats = get_valid_stats(trainer, args, extra_meters)
+
         for k, meter in extra_meters.items():
-            stats[k] = meter.avg
+            if k != 'sample_size':
+                stats[k] = meter.avg
         progress.print(stats, tag=subset, step=trainer.get_num_updates())
 
         valid_losses.append(
@@ -255,6 +264,7 @@ def validate(args, trainer, task, epoch_itr, subsets):
             if args.best_checkpoint_metric == 'loss'
             else stats[args.best_checkpoint_metric]
         )
+        del itr.itr
     return valid_losses
 
 
